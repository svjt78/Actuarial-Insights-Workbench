# Actuarial Insights Workbench

A comprehensive actuarial analytics platform for Commercial Property insurance, combining modern ML/AI capabilities with proven actuarial methodologies.

## ðŸŽ¯ Purpose

This platform demonstrates how actuarial, underwriting, and analytics workflows can be modernized through:
- **Loss Development Analysis** - Track emergence patterns with development triangles
- **Predictive Modeling** - ML-powered loss ratio and severity predictions
- **Segment Analytics** - Deep-dive KPIs across multiple dimensions
- **GenAI Insights** - Natural language explanations using OpenAI GPT-3.5

## âœ¨ Features

### 1. Loss Development Dashboard
- Cumulative and incremental loss triangles
- Age-to-age development factors
- Ultimate loss projections and IBNR estimation
- Monthly granularity with 36-month development
- Interactive heatmaps and visualizations

### 2. Pricing & Portfolio KPIs
- Segment analysis by Geography, Industry, Policy Size, Risk Rating
- Key metrics: Loss Ratio, Frequency, Severity, Pure Premium
- Frequency vs Severity scatter analysis
- Top/bottom performer identification
- Downloadable reports

### 3. Risk Prediction
- Loss Ratio prediction using LightGBM
- Claim Severity prediction using LightGBM
- Confidence intervals and uncertainty quantification
- Feature importance analysis
- Interactive risk scoring

### 4. GenAI Insights
- Natural language Q&A about portfolio metrics
- Loss ratio explanations with context
- Trend analysis and interpretation
- COPE risk rating explanations
- Powered by OpenAI GPT-3.5-turbo

## ðŸ§± Architecture

See [ARCHITECTURE.md](ARCHITECTURE.md) for detailed system design.

**Tech Stack:**
- **Frontend**: Streamlit, Plotly, Matplotlib
- **Backend**: FastAPI, Uvicorn
- **ML**: LightGBM, scikit-learn
- **GenAI**: OpenAI GPT-3.5-turbo
- **Data**: Pandas, NumPy
- **Infrastructure**: Docker, Docker Compose

## ðŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose installed
- OpenAI API key
- 8GB RAM minimum
- Ports 8000 and 8501 available

### Installation

1. **Clone the repository**
```bash
git clone <repository-url>
cd actuarial-insights-workbench
```

2. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env and add your OpenAI API key
nano .env
```

Add to `.env`:
```
OPENAI_API_KEY=your_openai_api_key_here
BACKEND_HOST=backend
BACKEND_PORT=8000
ENVIRONMENT=development
```

3. **Build and start services**
```bash
docker-compose up --build
```

This will:
- Build the backend and frontend Docker containers
- Start the FastAPI backend on `http://localhost:8000`
- Start the Streamlit frontend on `http://localhost:8501`

4. **Access the application**
- **Frontend**: http://localhost:8501
- **Backend API**: http://localhost:8000
- **API Docs**: http://localhost:8000/docs

### First-Time Setup

After starting the services, you need to train the ML models:

1. **Enter the backend container**
```bash
docker exec -it aiw-backend bash
```

2. **Train the models**
```bash
cd /app
python -c "import sys; sys.path.insert(0, '..'); exec(open('../scripts/train_models.py').read())"
```

3. **Verify models are created**
```bash
ls -la models/
# Should see: lr_model.pkl and severity_model.pkl
```

4. **Restart the backend** to load models
```bash
docker-compose restart backend
```

## ðŸ“Š Data

The platform uses synthetic Commercial Property insurance data:
- **1,000 policies** across 3 accident years (2022-2024)
- **100-200 claims** with realistic loss patterns
- **17,000+ exposure records** with monthly granularity

Data is generated using actuarially-sound methods with:
- COPE-based risk ratings (1-10 scale)
- Geographic and industry segmentation
- Realistic severity distributions
- Proper loss development patterns

To regenerate data:
```bash
cd scripts
python generate_data.py
```

## ðŸ›  Development

### Local Development (without Docker)

1. **Install Python 3.11+**

2. **Install dependencies**
```bash
pip install -r requirements.txt
```

3. **Generate data**
```bash
cd scripts
python generate_data.py
cd ..
```

4. **Train models**
```bash
cd scripts
python train_models.py
cd ..
```

5. **Start backend**
```bash
cd backend
uvicorn main:app --reload --port 8000
```

6. **Start frontend** (in new terminal)
```bash
cd frontend
streamlit run app.py --server.port 8501
```

### Project Structure

```
actuarial-insights-workbench/
â”œâ”€â”€ README.md                   # This file
â”œâ”€â”€ VISION.md                   # Project vision
â”œâ”€â”€ ARCHITECTURE.md             # Technical architecture
â”œâ”€â”€ docker-compose.yml          # Docker orchestration
â”œâ”€â”€ .env.example                # Environment template
â”œâ”€â”€ requirements.txt            # Python dependencies
â”‚
â”œâ”€â”€ backend/                    # FastAPI backend
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ main.py                 # API endpoints
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ loss_triangle.py    # Loss development logic
â”‚   â”‚   â”œâ”€â”€ segment_kpis.py     # KPI calculations
â”‚   â”‚   â”œâ”€â”€ prediction.py       # ML predictions
â”‚   â”‚   â””â”€â”€ explain.py          # GenAI explanations
â”‚   â”œâ”€â”€ models/                 # Trained ML models
â”‚   â”‚   â”œâ”€â”€ lr_model.pkl
â”‚   â”‚   â””â”€â”€ severity_model.pkl
â”‚   â””â”€â”€ tests/                  # Unit tests
â”‚
â”œâ”€â”€ frontend/                   # Streamlit UI
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ app.py                  # Landing page
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ 1_Loss_Development.py
â”‚       â”œâ”€â”€ 2_Pricing_KPIs.py
â”‚       â”œâ”€â”€ 3_Risk_Prediction.py
â”‚       â””â”€â”€ 4_GenAI_Insights.py
â”‚
â”œâ”€â”€ data/                       # Generated datasets
â”‚   â”œâ”€â”€ policies.csv
â”‚   â”œâ”€â”€ claims.csv
â”‚   â””â”€â”€ exposure.csv
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ generate_data.py        # Data generation
â”‚   â””â”€â”€ train_models.py         # Model training
â”‚
â””â”€â”€ notebooks/                  # Jupyter notebooks
    â”œâ”€â”€ 1_data_exploration.ipynb
    â”œâ”€â”€ 2_loss_triangle_analysis.ipynb
    â””â”€â”€ 3_model_development.ipynb
```

## ðŸ“– API Documentation

### Endpoints

**Predictions:**
- `POST /predict/loss_ratio` - Predict expected loss ratio
- `POST /predict/severity` - Predict claim severity
- `POST /predict/both` - Both predictions

**Analytics:**
- `GET /segment_insights` - Segment-level KPIs
- `GET /loss_triangle` - Loss development triangle

**GenAI:**
- `POST /explain` - Generate natural language explanations

**Utility:**
- `GET /health` - Health check
- `GET /data_summary` - Data summary statistics
- `GET /feature_importance/{model_type}` - Feature importance

Full API documentation available at: http://localhost:8000/docs

## ðŸ§ª Testing

Run unit tests:
```bash
cd backend
pytest tests/ -v --cov=services
```

## ðŸ”’ Security Notes

- Never commit `.env` file with real API keys
- Use environment variables for all secrets
- In production, implement proper authentication
- Restrict CORS origins in production
- Use HTTPS for all external communications

## ðŸ“ˆ Performance

- Backend handles ~100 requests/second
- Frontend supports multiple concurrent users
- Models load in <1 second
- Typical prediction latency: <100ms
- GenAI responses: 2-5 seconds

## ðŸŽ“ Use Cases

1. **Actuarial Analysis** - Reserve setting, loss development monitoring
2. **Underwriting** - Risk assessment, pricing guidance
3. **Portfolio Management** - Segment performance, profitability analysis
4. **Decision Support** - AI-powered insights and recommendations

## ðŸ—º Vision

See [VISION.md](VISION.md) for strategic goals and future capabilities.

## ðŸ¤ Contributing

This is a demonstration/educational project. For production use, consider:
- Adding user authentication
- Implementing database persistence
- Enhanced error handling
- Comprehensive logging
- Performance optimization
- Security hardening

## ðŸ“ž Contact

Created by Suvojit Dutta
Email: suvojit.dutta@zensar.com

## ðŸ“„ License

This project is for demonstration and educational purposes.

---

**Built with modern actuarial science, machine learning, and AI**
